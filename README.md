This code base was created as part of my thesis during the third year Discrete Mathematics Project at the University of Warwick, https://warwick.ac.uk/fac/sci/dcs/teaching/modules/cs344/. 

All experiments that are included in the thesis can be found under the 'notebooks' folder. All code is written in python, and dependencies are provided in the 'requirements.txt' file. The code for the Gaussian Mixture Models was created solely by the author, and all experiments of Chapters 3 and 4, as well as further experiments that were not included in the thesis, due to space constraints can be found in the folder '/notebooks/Gaussian_Mixture_Models/', were uniquely created for this thesis. 

For the Bayesian Neural Network Experiments of Chapter 5, we utilise the code base of Ashman et al. (2022), which can be freely used under the MIT license, see 'pvi/pvi_readme_files/LICENSE.txt'. We have modified their code base for the Bayesian Neural Network (BNN) experiments of Chapter 5 of the thesis, to include the Alpha-RÃ©nyi divergence we use to demonstrate Partitioned Generalised Variational Inference. This change is highlighted in Appendix C.4 of the thesis. Furthermore, we have fixed some bugs that have hindered the experiments that we carried out using their code base. We include the entire code base here, in order to make it easier for the reader to verify the results we provide. Furthermore, the format of the notebooks for the BNN experiments, is also due to Ashman et al. (2022), but the actual experiments provided in '/notebooks/Bayesian_Neural_Networks/' is unique to this thesis, where we have extended the previous experiments to different data sets, and different settings. 

Running the Gaussian Mixture Model experiments is straight forward as the data is generated locally in each file. We recommend running experiments from top to bottom if the reader whished to do so, if only some experiments are of interest for the Gaussian Mixture Models, then we note that it is necessary to run all auxiliary code provided in the notebooks until the first experiment is carried out, so that the PVI and PGVI functions are defined. We further remark that the functions are still called Partitioned Variational Inference or PVI, even if we are running PGVI; this is because, as we discuss in Chapter 4.4, we can modify an existing PVI code base to implement PGVI by changing the divergence measure, and the loss function. 

The BNN experiments require downloading larger data sets from the PyTorch Data Set repository or from NumPy in the case of the Kuzushiji-49 data set. This is automatically done in the code of the PGVI experiments and stored in '/data/', but it requires several hundred megabyte of storage. Running the PGVI BNN experiments is otherwise straightforward, but can be time intensive, as shown in the figures of Chapter 5.2.

We furthermore provide some other code that was not used throughout the thesis in the folder '/notebooks/Further_BNNs_NOT_used/'. This includes toy data sets for PVI with logistic regression, and Sparse Gaussian Processes. We also provide code for the Banana classification data set with Sparse Gaussian Process classification; it is not necessary to download this data, as we provide it in '/data/banana/'. 
We provide these experiments here, even though they are not used in the thesis, since they were not working in the original PVI code base of Ashman et al. (2022) to demonstrate that we fixed errors and bugs in their code. We further extend these experiments to different toy data sets. These are however not part of the thesis, since the objective of the project changed throughout the year, where we were moving away from doing simply PVI to developing a robust algorithm for model misspecification, PGVI.

We hope that this code can benefit others that are interested in robust Federated Learning.

References:

Matthew Ashman, Thang D. Bui, Cuong V. Nguyen, Stratis Markou, Adrian Weller, Siddharth Swaroop, and Richard E. Turner. Partitioned variational inference: A framework for probabilistic federated learning. arXiv preprint arXiv:2202.12275, 2022.
